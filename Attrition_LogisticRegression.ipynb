{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b18f893f-0504-4e8c-a373-237299c251de",
   "metadata": {},
   "source": [
    "## 0 - Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f3e20d0-94ab-4988-bb16-6dc39ea17445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db2798-6bf1-448f-8c7e-18550da78a70",
   "metadata": {},
   "source": [
    "## 1 - Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7aab08f-1dfe-42dd-85e8-92130098675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"WA_Fn-UseC_-HR-Employee-Attrition.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4142a5c1-3a43-4666-86e1-293e657a9188",
   "metadata": {},
   "source": [
    "## 2 - Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73cbdca3-69d4-4010-a304-e62e0c039c50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATASET OVERVIEW ===\n",
      "\n",
      "\n",
      "##### First 5 Rows #####\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EmployeeCount</th>\n",
       "      <th>EmployeeNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <th>StandardHours</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1102</td>\n",
       "      <td>Sales</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>279</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1373</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>1392</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>591</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Attrition     BusinessTravel  DailyRate              Department  \\\n",
       "0   41       Yes      Travel_Rarely       1102                   Sales   \n",
       "1   49        No  Travel_Frequently        279  Research & Development   \n",
       "2   37       Yes      Travel_Rarely       1373  Research & Development   \n",
       "3   33        No  Travel_Frequently       1392  Research & Development   \n",
       "4   27        No      Travel_Rarely        591  Research & Development   \n",
       "\n",
       "   DistanceFromHome  Education EducationField  EmployeeCount  EmployeeNumber  \\\n",
       "0                 1          2  Life Sciences              1               1   \n",
       "1                 8          1  Life Sciences              1               2   \n",
       "2                 2          2          Other              1               4   \n",
       "3                 3          4  Life Sciences              1               5   \n",
       "4                 2          1        Medical              1               7   \n",
       "\n",
       "   ...  RelationshipSatisfaction StandardHours  StockOptionLevel  \\\n",
       "0  ...                         1            80                 0   \n",
       "1  ...                         4            80                 1   \n",
       "2  ...                         2            80                 0   \n",
       "3  ...                         3            80                 0   \n",
       "4  ...                         4            80                 1   \n",
       "\n",
       "   TotalWorkingYears  TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n",
       "0                  8                      0               1               6   \n",
       "1                 10                      3               3              10   \n",
       "2                  7                      3               3               0   \n",
       "3                  8                      3               3               8   \n",
       "4                  6                      3               3               2   \n",
       "\n",
       "  YearsInCurrentRole  YearsSinceLastPromotion  YearsWithCurrManager  \n",
       "0                  4                        0                     5  \n",
       "1                  7                        1                     7  \n",
       "2                  0                        0                     0  \n",
       "3                  7                        3                     0  \n",
       "4                  2                        2                     2  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Last 5 Rows #####\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EmployeeCount</th>\n",
       "      <th>EmployeeNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <th>StandardHours</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>36</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>884</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>2061</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>39</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>613</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>2062</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>27</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>155</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>2064</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>49</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>1023</td>\n",
       "      <td>Sales</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>2065</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>34</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>628</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>2068</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age Attrition     BusinessTravel  DailyRate              Department  \\\n",
       "1465   36        No  Travel_Frequently        884  Research & Development   \n",
       "1466   39        No      Travel_Rarely        613  Research & Development   \n",
       "1467   27        No      Travel_Rarely        155  Research & Development   \n",
       "1468   49        No  Travel_Frequently       1023                   Sales   \n",
       "1469   34        No      Travel_Rarely        628  Research & Development   \n",
       "\n",
       "      DistanceFromHome  Education EducationField  EmployeeCount  \\\n",
       "1465                23          2        Medical              1   \n",
       "1466                 6          1        Medical              1   \n",
       "1467                 4          3  Life Sciences              1   \n",
       "1468                 2          3        Medical              1   \n",
       "1469                 8          3        Medical              1   \n",
       "\n",
       "      EmployeeNumber  ...  RelationshipSatisfaction StandardHours  \\\n",
       "1465            2061  ...                         3            80   \n",
       "1466            2062  ...                         1            80   \n",
       "1467            2064  ...                         2            80   \n",
       "1468            2065  ...                         4            80   \n",
       "1469            2068  ...                         1            80   \n",
       "\n",
       "      StockOptionLevel  TotalWorkingYears  TrainingTimesLastYear  \\\n",
       "1465                 1                 17                      3   \n",
       "1466                 1                  9                      5   \n",
       "1467                 1                  6                      0   \n",
       "1468                 0                 17                      3   \n",
       "1469                 0                  6                      3   \n",
       "\n",
       "     WorkLifeBalance  YearsAtCompany YearsInCurrentRole  \\\n",
       "1465               3               5                  2   \n",
       "1466               3               7                  7   \n",
       "1467               3               6                  2   \n",
       "1468               2               9                  6   \n",
       "1469               4               4                  3   \n",
       "\n",
       "      YearsSinceLastPromotion  YearsWithCurrManager  \n",
       "1465                        0                     3  \n",
       "1466                        1                     7  \n",
       "1467                        0                     3  \n",
       "1468                        0                     8  \n",
       "1469                        1                     2  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##########\n",
      "\n",
      "Shape: (1470, 35)\n",
      "\n",
      "Data Types:\n",
      " Age                          int64\n",
      "Attrition                   object\n",
      "BusinessTravel              object\n",
      "DailyRate                    int64\n",
      "Department                  object\n",
      "DistanceFromHome             int64\n",
      "Education                    int64\n",
      "EducationField              object\n",
      "EmployeeCount                int64\n",
      "EmployeeNumber               int64\n",
      "EnvironmentSatisfaction      int64\n",
      "Gender                      object\n",
      "HourlyRate                   int64\n",
      "JobInvolvement               int64\n",
      "JobLevel                     int64\n",
      "JobRole                     object\n",
      "JobSatisfaction              int64\n",
      "MaritalStatus               object\n",
      "MonthlyIncome                int64\n",
      "MonthlyRate                  int64\n",
      "NumCompaniesWorked           int64\n",
      "Over18                      object\n",
      "OverTime                    object\n",
      "PercentSalaryHike            int64\n",
      "PerformanceRating            int64\n",
      "RelationshipSatisfaction     int64\n",
      "StandardHours                int64\n",
      "StockOptionLevel             int64\n",
      "TotalWorkingYears            int64\n",
      "TrainingTimesLastYear        int64\n",
      "WorkLifeBalance              int64\n",
      "YearsAtCompany               int64\n",
      "YearsInCurrentRole           int64\n",
      "YearsSinceLastPromotion      int64\n",
      "YearsWithCurrManager         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def check_df(df):\n",
    "    print(\"\\n=== DATASET OVERVIEW ===\\n\")\n",
    "\n",
    "    print(\"\\n##### First 5 Rows #####\")\n",
    "    display(df.head(5))\n",
    "    \n",
    "    print(\"\\n##### Last 5 Rows #####\")\n",
    "    display(df.tail(5))\n",
    "\n",
    "    # Shape and Dtypes\n",
    "    print(\"\\n##########\")\n",
    "    print(\"\\nShape:\", df.shape)\n",
    "    print(\"\\nData Types:\\n\", df.dtypes)\n",
    "\n",
    "\n",
    "check_df(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925e1e84-e198-4c6a-977d-80d1d4f0e457",
   "metadata": {},
   "source": [
    "### HISTOGRAM - Distributions of Numeric Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1be0af-f187-4848-be45-dee154fd21a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [col for col in data.columns if data[col].dtypes != \"O\"]\n",
    "\n",
    "def plot_numeric_distributions(df, num_cols):\n",
    "    \"\"\"\n",
    "    Displays histograms of numerical variables side by side using subplots.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataset to visualize.\n",
    "    num_cols : list\n",
    "        List of numerical variable names.\n",
    "    \"\"\"\n",
    "    n = len(num_cols)\n",
    "    cols = 4  # Number of plots per row\n",
    "    rows = (n // cols) + 1\n",
    "\n",
    "    plt.figure(figsize=(15, rows * 4))\n",
    "\n",
    "    # Generate a color palette with distinct colors\n",
    "    colors = sns.color_palette(\"husl\", n)  # 'husl' palette for distinct colors\n",
    "\n",
    "    for i, col in enumerate(num_cols, 1):\n",
    "        plt.subplot(rows, cols, i)\n",
    "        df[col].hist(bins=30, edgecolor='black', color=colors[i-3])  # Assign a unique color to each histogram\n",
    "        plt.title(col)\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"Frequency\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_numeric_distributions(data, num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bba414f-6c1c-45eb-8844-0d53879c82fc",
   "metadata": {},
   "source": [
    "### HISTOGRAM - Distributions of Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6173a3d6-a0e6-4f10-94a6-4cc1ee18e70a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat_cols = [col for col in data.columns if data[col].dtypes == \"O\"]\n",
    "\n",
    "def plot_categoric_distributions(df, cat_cols, cols=3):\n",
    "    \"\"\"\n",
    "    Displays the distributions of numerical and categorical variables side by side using subplots.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataset to visualize.\n",
    "    num_cols : list, optional\n",
    "        List of numerical variable names.\n",
    "    cat_cols : list, optional\n",
    "        List of categorical variable names.\n",
    "    cols : int, default=3\n",
    "        Number of plots per row.\n",
    "    \"\"\"\n",
    "\n",
    "    # ðŸ”¹ Distribution of categorical variables\n",
    "    if cat_cols:\n",
    "        n = len(cat_cols)\n",
    "        rows = (n // cols) + 1\n",
    "        plt.figure(figsize=(15, rows * 4))\n",
    "\n",
    "        for i, col in enumerate(cat_cols, 1):\n",
    "            plt.subplot(rows, cols, i)\n",
    "            # Get unique categories for the current column\n",
    "            order = df[col].value_counts().index.unique()\n",
    "            \n",
    "            # Generate a list of distinct colors for each category\n",
    "            colors = sns.color_palette(\"husl\", len(order))  # 'husl' palette provides distinct colors\n",
    "            \n",
    "            # Use the same column for hue to assign colors to each category\n",
    "            sns.countplot(x=df[col], hue=df[col], order=order, palette=colors)\n",
    "            plt.title(col)\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            # Remove the legend\n",
    "            plt.legend([],[], frameon=False)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "plot_categoric_distributions(data, cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0073ed-1a7b-4030-a8d4-14b91d41de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop edilecek sabit sÃ¼tunlar\n",
    "columns_to_drop = ['EmployeeCount', 'Over18', 'StandardHours']\n",
    "\n",
    "data_ = data.copy()\n",
    "\n",
    "# SÃ¼tunlarÄ± drop et\n",
    "data_ = data_.drop(columns=columns_to_drop)\n",
    "\n",
    "# Sonucu kontrol et\n",
    "print(\"Drop edilen sÃ¼tunlar:\", columns_to_drop)\n",
    "print(\"Kalan sÃ¼tun sayÄ±sÄ±:\", data_.shape[1])\n",
    "#print(\"Kalan sÃ¼tun isimleri:\", data_.columns.tolist())\n",
    "\n",
    "# Ä°stersen yeni haliyle kaydet\n",
    "# df.to_csv('cleaned_attrition_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9149b83e-f368-438e-8bd8-3b5d0c4568e4",
   "metadata": {},
   "source": [
    "### Automatic Column Type Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2539aa40-4a0d-4655-9969-ed268072a0ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This function classifies DataFrame columns into:\n",
    "# - Categorical (cat_cols)\n",
    "# - Cardinal (high-unique-value categorical) (cat_but_car)\n",
    "# - Numerical (num_cols)\n",
    "# It uses thresholds for flexibility and supports visualization & detailed logging.\n",
    "# =============================================\n",
    "\n",
    "def grab_col_names(df, cat_th=13, car_th=20, plot=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Separates the variables in the given DataFrame into numeric and categorical groups.\n",
    "   \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The pandas DataFrame to be analyzed.\n",
    "    cat_th : int, optional\n",
    "        Max unique values to be considered categorical (default: 13).\n",
    "    car_th : int, optional\n",
    "        Min unique values to be considered cardinal (default: 20).\n",
    "    plot : bool, optional\n",
    "        Whether to show histogram of unique value counts (default: False).\n",
    "    verbose : bool, optional\n",
    "        Whether to print detailed logs and show classification table (default: False).\n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    cat_cols : list\n",
    "        Categorical variable names.\n",
    "    cat_but_car : list\n",
    "        Cardinal categorical variable names.\n",
    "    num_cols : list\n",
    "        Numerical variable names.\n",
    "    \"\"\"\n",
    "    # 1. Compute and sort the number of unique values per column\n",
    "    unique_counts = df.nunique().sort_values()\n",
    "    \n",
    "    # Print summary statistics of unique counts if verbose is enabled\n",
    "    if verbose:\n",
    "        print(\"Unique Value Summary:\")\n",
    "        print(unique_counts.describe(), \"\\n\")\n",
    "    \n",
    "    # 2. Plot histogram of unique value distribution (optional)\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(unique_counts, bins=30, edgecolor='black', alpha=0.7)\n",
    "        plt.axvline(cat_th, color='red', linestyle='--', label=f\"cat_th = {cat_th}\")\n",
    "        plt.axvline(car_th, color='green', linestyle='--', label=f\"car_th = {car_th}\")\n",
    "        plt.title(f\"Distribution of Unique Value Counts\\n(Data: {df.shape[0]} Rows, {df.shape[1]} Columns)\")\n",
    "        plt.xlabel(\"Number of Unique Values\")\n",
    "        plt.ylabel(\"Number of Columns\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # 3. Classify columns based on dtype and unique value counts\n",
    "    # Object-type columns â†’ potential categorical\n",
    "    cat_cols = [col for col in df.columns if df[col].dtypes == \"O\"]\n",
    "    \n",
    "    # Numeric columns with few unique values â†’ treat as categorical (e.g., rating 1â€“5)\n",
    "    num_but_cat = [col for col in df.columns if df[col].nunique() < cat_th and df[col].dtypes != \"O\"]\n",
    "    \n",
    "    # Object-type columns with many unique values â†’ cardinal (e.g., IDs, codes)\n",
    "    cat_but_car = [col for col in df.columns if df[col].nunique() > car_th and df[col].dtypes == \"O\"]\n",
    "    \n",
    "    # Combine object categoricals with numeric-but-categorical\n",
    "    cat_cols = cat_cols + num_but_cat\n",
    "    \n",
    "    # Remove cardinal columns from categorical list\n",
    "    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n",
    "    \n",
    "    # All non-object columns â†’ initially numeric\n",
    "    num_cols = [col for col in df.columns if df[col].dtypes != \"O\"]\n",
    "    \n",
    "    # Remove numeric-but-categorical from numeric list\n",
    "    num_cols = [col for col in num_cols if col not in num_but_cat]\n",
    "    \n",
    "    # 4. Detailed output when verbose is True\n",
    "    if verbose:\n",
    "        print(f\"num_but_cat: {num_but_cat}\")\n",
    "        print(f\"cat_but_car: {cat_but_car}\")\n",
    "        print(f\"cat_cols: {cat_cols}\")\n",
    "        print(f\"num_cols: {num_cols}\")\n",
    "        print(f\"\\nObservations: {df.shape[0]}\")\n",
    "        print(f\"Variables: {df.shape[1]}\")\n",
    "        print(f\"Categorical variables: {len(cat_cols)}\")\n",
    "        print(f\"Numeric variables: {len(num_cols)}\")\n",
    "        print(f\"Cardinal categorical variables: {len(cat_but_car)}\")\n",
    "        print(f\"Numeric but categorical-like: {len(num_but_cat)}\")\n",
    "        \n",
    "        # Build and display classification table\n",
    "        result = pd.DataFrame({\n",
    "            \"Unique_Count\": unique_counts,\n",
    "            \"Type\": [\n",
    "                \"Categorical\" if col in cat_cols else\n",
    "                \"Cardinal\" if col in cat_but_car else\n",
    "                \"Numeric\"\n",
    "                for col in unique_counts.index\n",
    "            ]\n",
    "        })\n",
    "        print(\"\\nVariable Classification:\\n\")\n",
    "        display(result)\n",
    "    \n",
    "    # Return the three column groups\n",
    "    return cat_cols, cat_but_car, num_cols, num_but_cat\n",
    "\n",
    "\n",
    "cat_cols, cat_but_car, num_cols, num_but_cat = grab_col_names(data_, plot=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975971c8-3a12-44e4-85f6-9d4904026dfe",
   "metadata": {},
   "source": [
    "### MISSING VALUE - Detect Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03038d22-6852-481d-b554-7a726e609bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive list of values that should be treated as missing\n",
    "missing_indicators = [\n",
    "    np.nan, None, pd.NA,                           # Real missing-value objects\n",
    "    \"\", \" \", \"-\", \"--\", \"---\", \"?\", \"??\", \"???\",   # Empty strings, whitespace, dashes, question marks\n",
    "    \"NA\", \"N/A\", \"n/a\", \"na\", \"Na\", \"na.\", \"N.A\", \"n.a.\",  # All variants of \"NA\"\n",
    "    \"Missing\", \"MISSING\", \"missing\",               # \"Missing\" text\n",
    "    \"Null\", \"NULL\", \"null\",                        # \"Null\" variants\n",
    "    \"None\", \"NONE\", \"none\",                        # \"None\" text\n",
    "    # \"0\", \"000\", \"999\", \"9999\", \"-999\", \"-9999\",  # Commented out â€“ could be real data, not safe to replace\n",
    "    \"unknown\", \"Unknown\", \"UNKNOWN\",               # \"unknown\" variants\n",
    "    \"unavailable\", \"Unavail\", \"UNAVAILABLE\",\n",
    "    \"not applicable\", \"Not applicable\", \"NOT APPLICABLE\",\n",
    "    \"not recorded\", \"Not Recorded\", \"NOT RECORDED\",\n",
    "    \"not provided\", \"Not Provided\", \"NOT PROVIDED\",\n",
    "    \"not known\", \"Not Known\", \"NOT KNOWN\"\n",
    "]\n",
    "\n",
    "# Function: Detect, count and explain missing values by type\n",
    "def check_and_explain_missing(df):\n",
    "    print(\"Checking for missing values...\\n\")\n",
    "    df_copy = df.copy()                            # Work on a copy â€“ never modify the original\n",
    "    \n",
    "    results = {}                                   # Dictionary to store per-column results\n",
    "    \n",
    "    for col in df.columns:                         # Loop over every column\n",
    "        col_data = df[col]                         # Raw column data\n",
    "        # Convert to string, strip whitespace, lower-case â†’ normalises text for matching\n",
    "        col_str = col_data.astype(str).str.strip().str.lower()\n",
    "        \n",
    "        # Lower-case version of string-based missing indicators\n",
    "        indicators_lower = [str(x).lower() for x in missing_indicators if isinstance(x, str)]\n",
    "        \n",
    "        # Count true NaN-like missing values (separately for clarity)\n",
    "        nan_types = {\n",
    "            #  pd.NA â†’ only object, numeric â†’ np.nan\n",
    "            \"np.nan\": col_data.isna().sum(),       # Covers NaN, None, pd.NA, etc.\n",
    "            \"None\": (col_data == None).sum(),      # Python None\n",
    "            \"pd.NA\": (col_data == pd.NA).sum() if col_data.dtype == \"object\" else 0  # Pandas scalar NA\n",
    "        }\n",
    "        nan_like_total = sum(nan_types.values())   # Total NaN-type missing\n",
    "        \n",
    "        # Find rows that match any string-based missing_indicator\n",
    "        string_like_mask = col_str.isin(indicators_lower)  # Does the column contain missing-value strings (\"na\", \"unknown\", etc.)? â†’ True/False mask\n",
    "        string_like_total = string_like_mask.sum() # How many string-based missing values\n",
    "        \n",
    "        total_missing = nan_like_total + string_like_total  # Grand total missing in this column\n",
    "        \n",
    "        # Store details only if there is at least one missing value\n",
    "        if total_missing > 0:\n",
    "            # Actual string values that were found (original case)\n",
    "            string_values_found = col_data[string_like_mask].unique().tolist()\n",
    "            # Which NaN-like types were present\n",
    "            nan_values_found = [k for k, v in nan_types.items() if v > 0]\n",
    "            \n",
    "            results[col] = {\n",
    "                \"Total Missing\": total_missing,\n",
    "                \"NaN-like Total\": nan_like_total,\n",
    "                \"String-like Total\": string_like_total,\n",
    "                \"NaN-like Types\": nan_values_found,\n",
    "                \"String-like Types\": string_values_found\n",
    "            }\n",
    "    \n",
    "    # Print a nicely formatted report\n",
    "    if results:\n",
    "        print(\"Missing values detected:\\n\")\n",
    "        for col, info in results.items():\n",
    "            print(f\" {col}: {info['Total Missing']} total missing\")\n",
    "            print(f\" â€¢ NaN-like: {info['NaN-like Total']}\")\n",
    "            if info['NaN-like Types']:\n",
    "                print(f\" Found NaN missing types: {info['NaN-like Types']}\")\n",
    "            print(f\" â€¢ String-like: {info['String-like Total']}\")\n",
    "            if info['String-like Types']:\n",
    "                print(f\"   Found string missing types: {info['String-like Types']}\")\n",
    "            print()  # blank line between columns\n",
    "    else:\n",
    "        print(\"No missing values found in any column.\")\n",
    "        return results\n",
    "\n",
    "\n",
    "missing_report= check_and_explain_missing(data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a107cf-73bf-42e3-a61a-a2daa1229ef3",
   "metadata": {},
   "source": [
    "### OUTLIERS AND RARE - Detect Outliers & Rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5341fdf0-3a2b-4ab6-84c2-ef2f600f797b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def detect_outliers_and_rare_categories(\n",
    "    df,\n",
    "    target=None,\n",
    "    exclude_cols=None,\n",
    "    num_but_cat=None,\n",
    "    rare_threshold=0.01,\n",
    "    low_quantile=0.25,\n",
    "    up_quantile=0.75,\n",
    "    iqr_multiplier=3.0,\n",
    "    use_log_transform=True,\n",
    "    skew_threshold=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Detects:\n",
    "    - Outliers in continuous numeric variables using IQR (optionally with log transform)\n",
    "    - Rare categories in nominal categorical variables\n",
    "\n",
    "    Numeric-but-categorical variables are excluded from both analyses.\n",
    "    \"\"\"\n",
    "\n",
    "    df_copy = df.copy()             # Work on a copy to avoid modifying original data\n",
    "\n",
    "    exclude_cols = exclude_cols or []   # Columns to fully exclude from analysis\n",
    "    num_but_cat = num_but_cat or []     # Numeric-looking categorical variables\n",
    "\n",
    "    # -----------------------------\n",
    "    # COLUMN SELECTION\n",
    "    # -----------------------------\n",
    "    # Select numeric columns\n",
    "    num_cols = df_copy.select_dtypes(include=['number']).columns.tolist()\n",
    "    \n",
    "    # Select categorical columns (object or category)\n",
    "    cat_cols = df_copy.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Remove excluded and numeric-but-categorical columns from numeric list\n",
    "    num_cols = [\n",
    "        c for c in num_cols\n",
    "        if c not in exclude_cols and c not in num_but_cat\n",
    "    ]\n",
    "\n",
    "    # Remove excluded and numeric-but-categorical columns from categorical list\n",
    "    cat_cols = [\n",
    "        c for c in cat_cols\n",
    "        if c not in exclude_cols and c not in num_but_cat\n",
    "    ]\n",
    "    \n",
    "    outlier_report = {}             # Store numeric outlier results\n",
    "    rare_report = {}                # Store rare category results\n",
    "\n",
    "    # ==================================================\n",
    "    # 1. NUMERIC OUTLIER DETECTION (CONTINUOUS ONLY)\n",
    "    # ==================================================\n",
    "    for col in num_cols:\n",
    "        series = df_copy[col].dropna()   # Remove missing values\n",
    "\n",
    "        if series.empty:                # Skip column if no valid data\n",
    "            continue\n",
    "\n",
    "        skew = series.skew()            # Calculate skewness\n",
    "        transformed = series            # Default: no transformation\n",
    "        transform_note = \"\"             # Keep track of transformation\n",
    "\n",
    "        # Apply log transform if distribution is highly skewed and values are positive\n",
    "        if (\n",
    "            use_log_transform\n",
    "            and skew > skew_threshold\n",
    "            and (series > 0).all()\n",
    "        ):\n",
    "            transformed = np.log1p(series)\n",
    "            transform_note = \"log1p applied\"\n",
    "\n",
    "        # Compute IQR boundaries\n",
    "        q_low = transformed.quantile(low_quantile)\n",
    "        q_up = transformed.quantile(up_quantile)\n",
    "        iqr = q_up - q_low\n",
    "\n",
    "        low_limit = q_low - iqr_multiplier * iqr\n",
    "        up_limit = q_up + iqr_multiplier * iqr\n",
    "\n",
    "        # Convert limits back to original scale if log transform was used\n",
    "        if transform_note:\n",
    "            low_limit = np.expm1(low_limit) if low_limit > 0 else low_limit\n",
    "            up_limit = np.expm1(up_limit)\n",
    "\n",
    "        # Identify outliers based on limits\n",
    "        mask = (df_copy[col] < low_limit) | (df_copy[col] > up_limit)\n",
    "\n",
    "        # Store indices and actual outlier values\n",
    "        outlier_indices = df_copy[mask].index.tolist()\n",
    "        outlier_values = df_copy.loc[outlier_indices, col].tolist()\n",
    "\n",
    "        # Save outlier information for this column\n",
    "        outlier_report[col] = {\n",
    "            \"type\": \"numeric_continuous\",\n",
    "            \"count\": int(mask.sum()),\n",
    "            \"percentage\": round(100 * mask.sum() / len(df_copy), 2),\n",
    "            \"lower_limit\": round(float(low_limit), 2),\n",
    "            \"upper_limit\": round(float(up_limit), 2),\n",
    "            \"skew\": round(float(skew), 2),\n",
    "            \"transform\": transform_note,\n",
    "            \"indices\": outlier_indices,\n",
    "            \"values\": outlier_values\n",
    "        }\n",
    "\n",
    "    # ==================================================\n",
    "    # 2. RARE CATEGORY DETECTION (NOMINAL ONLY)\n",
    "    # ==================================================\n",
    "    for col in cat_cols:\n",
    "        freq = df_copy[col].value_counts(normalize=True)   # Category frequencies\n",
    "        rare_cats = freq[freq < rare_threshold].index.tolist()  # Rare categories\n",
    "        rare_count = df_copy[col].isin(rare_cats).sum()   # Number of rare samples\n",
    "\n",
    "        target_means = None\n",
    "        if target and target in df_copy.columns:\n",
    "            # Mean of target variable per category (for interpretability)\n",
    "            target_means = (\n",
    "                df_copy.groupby(col)[target]\n",
    "                .mean()\n",
    "                .round(4)\n",
    "                .to_dict()\n",
    "            )\n",
    "\n",
    "        # Save rare category information\n",
    "        rare_report[col] = {\n",
    "            \"type\": \"categorical_nominal\",\n",
    "            \"total_categories\": len(freq),\n",
    "            \"rare_categories\": rare_cats,\n",
    "            \"rare_count\": int(rare_count),\n",
    "            \"rare_percentage\": round(100 * rare_count / len(df_copy), 2),\n",
    "            \"target_means\": target_means\n",
    "        }\n",
    "\n",
    "    # ==================================================\n",
    "    # 3. PRINT SUMMARY\n",
    "    # ==================================================\n",
    "    print(\"=\" * 80)\n",
    "    print(\" OUTLIER & RARE CATEGORY REPORT \".center(80))\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Print numeric outliers with indices and values\n",
    "    print(\"\\nNUMERIC CONTINUOUS OUTLIERS\".center(100))\n",
    "    print(\"-\" * 100)\n",
    "    if any(v[\"count\"] > 0 for v in outlier_report.values()):\n",
    "        for col, info in outlier_report.items():\n",
    "            if info[\"count\"] > 0:\n",
    "                indices_str = ', '.join(map(str, info['indices'][:10]))\n",
    "                values_str = ', '.join(map(str, [round(v, 2) for v in info['values'][:10]]))\n",
    "                print(f\"{col:25} â†’ {info['count']} ({info['percentage']}%) \"\n",
    "                      f\"[{info['lower_limit']}, {info['upper_limit']}] (skew: {info['skew']})\")\n",
    "                print(f\"    Indices : [{indices_str}]\")\n",
    "                print(f\"    Values  : [{values_str}]\")\n",
    "                print()\n",
    "    else:\n",
    "        print(\"No numeric outliers detected.\")\n",
    "\n",
    "    # Print rare categorical values\n",
    "    print(\"\\nRARE CATEGORIES (NOMINAL)\".center(80))\n",
    "    print(\"-\" * 80)\n",
    "    if any(v[\"rare_count\"] > 0 for v in rare_report.values()):\n",
    "        for col, info in rare_report.items():\n",
    "            if info[\"rare_count\"] > 0:\n",
    "                print(\n",
    "                    f\"{col:22} â†’ {info['rare_count']} \"\n",
    "                    f\"({info['rare_percentage']}%) \"\n",
    "                    f\"Rare: {info['rare_categories']}\"\n",
    "                )\n",
    "    else:\n",
    "        print(\"No rare categories detected.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "    return outlier_report, rare_report\n",
    "\n",
    "\n",
    "# Convert target variable to numeric\n",
    "data_['Attrition_num'] = data_['Attrition'].map({'Yes': 1, 'No': 0}).astype(int)\n",
    "\n",
    "# Run outlier and rare category detection\n",
    "outlier_report, rare_report = detect_outliers_and_rare_categories(\n",
    "    df=data_,\n",
    "    target='Attrition_num',\n",
    "    exclude_cols=['Attrition', 'Attrition_num'],\n",
    "    num_but_cat=num_but_cat,\n",
    "    rare_threshold=0.05,\n",
    "    iqr_multiplier=3,\n",
    "    use_log_transform=True,\n",
    "    skew_threshold=1.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c42d0e2-49bb-4e3d-acb1-e9839766ec28",
   "metadata": {},
   "source": [
    "### Visualizes Detected Outliers and Rare Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56e717d-9904-4f05-bd05-939c53a83804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_outliers_and_rare(\n",
    "    df,                          # Original DataFrame\n",
    "    outlier_report,              # Dictionary containing numeric outlier information\n",
    "    rare_report,                 # Dictionary containing rare category information\n",
    "    max_outliers_show=20,        # Maximum number of outlier points to display per feature\n",
    "    rare_min_count=1             # Minimum count to display a category in bar plots\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualizes detected outliers and rare categories.\n",
    "    - Numeric outliers: Boxplot + highlighted scatter points\n",
    "    - Rare categories: Bar plots based on frequency\n",
    "    \"\"\"\n",
    "\n",
    "    sns.set_style(\"whitegrid\")   # Set seaborn plot style for better readability\n",
    "    \n",
    "    # -----------------------------\n",
    "    # 1. NUMERIC OUTLIERS - BOXPLOTS\n",
    "    # -----------------------------\n",
    "    if outlier_report:           # Check if numeric outlier results exist\n",
    "        \n",
    "        # Select numeric columns that actually contain outliers\n",
    "        num_outlier_cols = [\n",
    "            col for col, info in outlier_report.items() if info['count'] > 0\n",
    "        ]\n",
    "        \n",
    "        if num_outlier_cols:     # Continue only if at least one column has outliers\n",
    "            print(\"ðŸ“Š Numeric Outliers - Boxplots\")\n",
    "            \n",
    "            # Create subplots dynamically based on number of columns\n",
    "            fig, axes = plt.subplots(\n",
    "                1,\n",
    "                len(num_outlier_cols),\n",
    "                figsize=(6 * len(num_outlier_cols), 6)\n",
    "            )\n",
    "            \n",
    "            # Ensure axes is iterable when only one subplot exists\n",
    "            if len(num_outlier_cols) == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            # Loop through each numeric column with outliers\n",
    "            for ax, col in zip(axes, num_outlier_cols):\n",
    "                \n",
    "                # Draw boxplot for the column\n",
    "                sns.boxplot(\n",
    "                    y=df[col],\n",
    "                    ax=ax,\n",
    "                    color='lightblue'\n",
    "                )\n",
    "                \n",
    "                # Retrieve outlier metadata for this column\n",
    "                info = outlier_report[col]\n",
    "                \n",
    "                # Limit the number of displayed outliers\n",
    "                outlier_vals = info['values'][:max_outliers_show]\n",
    "                outlier_indices = info['indices'][:max_outliers_show]\n",
    "                \n",
    "                # Overlay outlier points as red dots\n",
    "                ax.scatter(\n",
    "                    x=np.zeros(len(outlier_vals)),  # Align dots vertically\n",
    "                    y=outlier_vals,\n",
    "                    color='red',\n",
    "                    s=50,\n",
    "                    label='Outlier'\n",
    "                )\n",
    "                \n",
    "                # Set informative plot title\n",
    "                ax.set_title(\n",
    "                    f\"{col}\\n\"\n",
    "                    f\"{info['count']} outliers ({info['percentage']}%)\\n\"\n",
    "                    f\"Limits: [{info['lower_limit']}, {info['upper_limit']}]\"\n",
    "                )\n",
    "                \n",
    "                ax.legend()      # Show legend\n",
    "                \n",
    "                # Apply log scale if log transformation was used\n",
    "                if info['transform'] == 'log1p applied':\n",
    "                    ax.set_yscale('log')\n",
    "                    ax.set_ylabel(f\"{col} (log scale)\")\n",
    "            \n",
    "            plt.tight_layout()   # Adjust spacing between plots\n",
    "            plt.show()           # Display the figure\n",
    "        \n",
    "        # -----------------------------\n",
    "        # 1b. SCATTER PLOT (IF 2+ NUMERIC OUTLIERS)\n",
    "        # -----------------------------\n",
    "        if len(num_outlier_cols) >= 2:\n",
    "            print(\"ðŸ“Š Relationship Between Numeric Outliers - Scatter Plot\")\n",
    "            \n",
    "            # Copy only numeric columns with outliers\n",
    "            scatter_df = df[num_outlier_cols].copy()\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            \n",
    "            # Plot all points in light color (normal observations)\n",
    "            plt.scatter(\n",
    "                scatter_df.iloc[:, 0],\n",
    "                scatter_df.iloc[:, 1],\n",
    "                alpha=0.3,\n",
    "                label='Normal'\n",
    "            )\n",
    "            \n",
    "            # Highlight outliers in red\n",
    "            for col in num_outlier_cols:\n",
    "                info = outlier_report[col]\n",
    "                outlier_idx = info['indices']\n",
    "                \n",
    "                plt.scatter(\n",
    "                    df.loc[outlier_idx, num_outlier_cols[0]],\n",
    "                    df.loc[outlier_idx, num_outlier_cols[1]],\n",
    "                    color='red',\n",
    "                    s=60,\n",
    "                    label=f'{col} Outlier' if col == num_outlier_cols[0] else \"\"\n",
    "                )\n",
    "            \n",
    "            plt.xlabel(num_outlier_cols[0])   # X-axis label\n",
    "            plt.ylabel(num_outlier_cols[1])   # Y-axis label\n",
    "            plt.title(\"Distribution of Outliers Across Numeric Features\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    \n",
    "    # -----------------------------\n",
    "    # 2. RARE CATEGORIES - BAR PLOTS\n",
    "    # -----------------------------\n",
    "    if rare_report:              # Check if rare category results exist\n",
    "        \n",
    "        # Select categorical columns that contain rare categories\n",
    "        rare_cols = [\n",
    "            col for col, info in rare_report.items() if info['rare_count'] > 0\n",
    "        ]\n",
    "        \n",
    "        if rare_cols:\n",
    "            print(\"ðŸ“Š Rare Categories - Bar Plots\")\n",
    "            \n",
    "            # Create subplots dynamically\n",
    "            fig, axes = plt.subplots(\n",
    "                1,\n",
    "                len(rare_cols),\n",
    "                figsize=(7 * len(rare_cols), 6)\n",
    "            )\n",
    "            \n",
    "            # Ensure axes is iterable\n",
    "            if len(rare_cols) == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            # Loop through each categorical column with rare values\n",
    "            for ax, col in zip(axes, rare_cols):\n",
    "                \n",
    "                # Count occurrences of each category\n",
    "                counts = df[col].value_counts()\n",
    "                \n",
    "                # Color rare categories red, others blue\n",
    "                colors = [\n",
    "                    'red' if cat in rare_report[col]['rare_categories']\n",
    "                    else 'skyblue'\n",
    "                    for cat in counts.index\n",
    "                ]\n",
    "                \n",
    "                # Draw bar plot\n",
    "                counts.plot(\n",
    "                    kind='bar',\n",
    "                    ax=ax,\n",
    "                    color=colors\n",
    "                )\n",
    "                \n",
    "                # Set informative title\n",
    "                ax.set_title(\n",
    "                    f\"{col}\\n\"\n",
    "                    f\"{len(rare_report[col]['rare_categories'])} rare categories\\n\"\n",
    "                    f\"Total rare rows: {rare_report[col]['rare_count']} \"\n",
    "                    f\"({rare_report[col]['rare_percentage']}%)\"\n",
    "                )\n",
    "                \n",
    "                ax.set_ylabel(\"Row Count\")     # Y-axis label\n",
    "                ax.tick_params(axis='x', rotation=45)  # Rotate labels\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# FUNCTION CALL\n",
    "# ------------------------------------------------------------\n",
    "visualize_outliers_and_rare(\n",
    "    df=data_,                    # DataFrame to visualize\n",
    "    outlier_report=outlier_report,  # Numeric outlier analysis results\n",
    "    rare_report=rare_report,        # Rare category analysis results\n",
    "    max_outliers_show=20             # Limit outlier visualization\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81884e7-dd71-4664-b88e-cd9b9bb4ad42",
   "metadata": {},
   "source": [
    "### Converts Rare Categories to 'Rare'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72303c0f-32b4-4b7a-998c-fe73e9a21125",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rare_only_merge(\n",
    "    df,                    # Input DataFrame\n",
    "    rare_report,           # Dictionary containing rare category analysis\n",
    "    num_but_cat=None       # Numeric-looking but categorical columns to exclude\n",
    "):\n",
    "    \"\"\"\n",
    "    Converts ONLY rare categories to 'Rare'.\n",
    "    Does NOT touch outliers â†’ meaningful extreme values are preserved.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()         # Create a copy to avoid modifying original DataFrame\n",
    "    num_but_cat = num_but_cat or []  # Ensure it's a list if None is given\n",
    "\n",
    "    summary = {\"rare\": {}} # Store summary of merged rare categories\n",
    "    total_merged = 0       # Counter for total merged rows\n",
    "\n",
    "    print(\"ðŸ”´ RARE CATEGORY MERGING STARTED (Outliers preserved!)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # ==================================================\n",
    "    # MERGE ONLY RARE CATEGORIES (BASED ON RARE REPORT)\n",
    "    # ==================================================\n",
    "    for col, info in rare_report.items():   # Loop over rare report entries\n",
    "        if (\n",
    "            col not in df.columns           # Skip if column not in DataFrame\n",
    "            or info[\"rare_count\"] == 0      # Skip if no rare categories\n",
    "            or col in num_but_cat           # Skip numeric-but-categorical columns\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        rare_cats = info[\"rare_categories\"] # List of rare category names\n",
    "        mask = df[col].isin(rare_cats)      # Boolean mask for rare categories\n",
    "        merged = int(mask.sum())             # Number of rows to be merged\n",
    "\n",
    "        if merged > 0:\n",
    "            df.loc[mask, col] = \"Rare\"       # Replace rare categories with 'Rare'\n",
    "            summary[\"rare\"][col] = (len(rare_cats), merged)  # Save merge info\n",
    "            total_merged += merged           # Update total merged count\n",
    "\n",
    "            print(\n",
    "                f\"{col:<25} â†’ {len(rare_cats):>2} categories \"\n",
    "                f\"({merged:>3} rows) â†’ merged as 'Rare'\"\n",
    "            )\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\n",
    "        f\"ðŸ“Š TOTAL: {total_merged} rows merged as rare \"\n",
    "        f\"(%{round(100 * total_merged / len(df), 2)})\"\n",
    "    )\n",
    "    print(\"YearsAtCompany and YearsSinceLastPromotion outliers are PRESERVED!\")\n",
    "    print(\"   â†’ Long-tenure & no-promotion patterns remain intact\")\n",
    "\n",
    "    # ==================================================\n",
    "    # SUMMARY REPORT\n",
    "    # ==================================================\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\" RARE MERGE SUMMARY \".center(70))\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\"\\nRARE CATEGORY MERGES\")\n",
    "    print(\"-\" * 70)\n",
    "    if summary[\"rare\"]:\n",
    "        for col, (cats, rows) in summary[\"rare\"].items():\n",
    "            print(\n",
    "                f\"{col:<30} â†’ {cats:>2} categories \"\n",
    "                f\"â†’ {rows:>4} rows â†’ 'Rare'\"\n",
    "            )\n",
    "    else:\n",
    "        print(\"No rare categories were merged.\")\n",
    "\n",
    "    print(\"\\nDATASET STATUS\")\n",
    "    print(\"-\" * 70)\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns  # Numeric columns\n",
    "    cat_cols = df.select_dtypes(include=\"object\").columns     # Categorical columns\n",
    "    print(\n",
    "        f\"Rows: {df.shape[0]:>4} | \"\n",
    "        f\"Numeric: {len(num_cols):>2} | \"\n",
    "        f\"Categorical: {len(cat_cols):>2}\"\n",
    "    )\n",
    "    print(f\"Columns with rare merging: {len(summary['rare'])}\")\n",
    "    print(\"\\nðŸŽ‰ Clean dataset ready for modeling!\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    return df               # Return cleaned DataFrame\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# USAGE (Rare merge ONLY â€” no outlier handling)\n",
    "# ==================================================\n",
    "df_clean = rare_only_merge(\n",
    "    df=data_,               # Original dataset\n",
    "    rare_report=rare_report,# Rare category analysis output\n",
    "    num_but_cat=num_but_cat # Excluded numeric-but-categorical columns\n",
    ")\n",
    "\n",
    "# Check if outliers are still preserved\n",
    "print(\"\\nðŸ” OUTLIER CHECK (should be preserved):\")\n",
    "print(\"YearsAtCompany max:\", df_clean[\"YearsAtCompany\"].max())              # Expected: 40\n",
    "print(\"YearsSinceLastPromotion max:\", df_clean[\"YearsSinceLastPromotion\"].max())  # Expected: 15\n",
    "\n",
    "# Verify 'Rare' category creation\n",
    "print(\"\\nðŸ” RARE CATEGORY CHECK:\")\n",
    "for col in [\"Department\", \"EducationField\", \"JobRole\"]:\n",
    "    if col in df_clean.columns:\n",
    "        print(\n",
    "            f\"{col}: Unique values = {df_clean[col].nunique()}, \"\n",
    "            f\"'Rare' exists? {('Rare' in df_clean[col].values)}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cba100-ccf1-4cb0-8994-77f9418a0f29",
   "metadata": {},
   "source": [
    "### Correlation Heatmap (Encoded Categorical + Numerical Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dffb1d-0a73-4af9-bcd6-8ec4ab72c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 1. Remove target variables\n",
    "# ============================\n",
    "\n",
    "df_corr = df_clean.drop(\n",
    "    columns=[\"Attrition_num\", \"Attrition\"]\n",
    ")  \n",
    "# Drop target columns to avoid target leakage in correlation analysis\n",
    "# Correlation should be computed ONLY on features\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 2. One-Hot Encoding (nominal categorical features)\n",
    "# ============================\n",
    "\n",
    "# Select categorical columns that:\n",
    "# - are not numeric-but-categorical\n",
    "# - are not the target variable\n",
    "ohe_cols = [\n",
    "    col for col in cat_cols \n",
    "    if col not in num_but_cat and col != \"Attrition\"\n",
    "]\n",
    "\n",
    "# Apply One-Hot Encoding to selected categorical columns\n",
    "df_corr = pd.get_dummies(\n",
    "    df_corr,          # DataFrame to encode\n",
    "    columns=ohe_cols, # Columns to be encoded\n",
    "    drop_first=True   # Drop first category to avoid dummy variable trap\n",
    ")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 3. Correlation matrix\n",
    "# ============================\n",
    "\n",
    "corr_matrix = df_corr.corr(\n",
    "    method=\"pearson\"\n",
    ")\n",
    "# Compute Pearson correlation\n",
    "# Measures linear relationships between features\n",
    "# Values range from -1 (negative) to +1 (positive)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 4. Correlation Heatmap (full view)\n",
    "# ============================\n",
    "\n",
    "plt.figure(figsize=(18, 14))  # Set figure size for better visibility\n",
    "\n",
    "sns.heatmap(\n",
    "    corr_matrix,              # Correlation matrix data\n",
    "    # annot=True,             # Uncomment to display correlation values\n",
    "    cmap=\"coolwarm\",          # Color map: blue (negative) â†’ red (positive)\n",
    "    center=0,                 # Center color scale at zero\n",
    "    linewidths=0.3,           # Add grid lines between cells\n",
    "    cbar_kws={\"shrink\": 0.8}  # Adjust color bar size\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    \"Correlation Heatmap (Encoded Categorical + Numerical Features)\",\n",
    "    fontsize=16\n",
    ")  # Set plot title\n",
    "\n",
    "plt.tight_layout()  # Automatically adjust spacing\n",
    "plt.show()          # Display the heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a14a438-ef08-40f9-b7d6-bdc246f39d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_pairs(corr, threshold):\n",
    "    upper = corr.where(\n",
    "        np.triu(np.ones(corr.shape), k=1).astype(bool)\n",
    "    )\n",
    "    return (upper.abs() >= threshold).sum().sum()\n",
    "\n",
    "for t in [0.5, 0.6, 0.7, 0.8]:\n",
    "    print(f\"Threshold {t}: {count_unique_pairs(corr_matrix, t)} unique pairs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899cfb1e-45e5-42f2-a89e-a517e343ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 1. Correlation matrix\n",
    "# ============================\n",
    "\n",
    "corr = corr_matrix.copy()\n",
    "# Create a copy of the correlation matrix to avoid modifying the original\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 2. Extract upper triangle (avoid duplicate pairs)\n",
    "# ============================\n",
    "\n",
    "upper_triangle = corr.where(\n",
    "    np.triu(np.ones(corr.shape), k=1).astype(bool)\n",
    ")\n",
    "# Keep only the upper triangle of the matrix\n",
    "# This prevents showing the same feature pair twice\n",
    "# k=1 removes the diagonal (self-correlation)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 3. Set correlation threshold\n",
    "# ============================\n",
    "\n",
    "threshold = 0.7\n",
    "# Define minimum absolute correlation value\n",
    "# Features with |correlation| >= threshold will be selected\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 4. Extract highly correlated feature pairs\n",
    "# ============================\n",
    "\n",
    "high_corr_pairs = (\n",
    "    upper_triangle.stack()   # Convert matrix to long (pairwise) format\n",
    "    .reset_index()           # Reset index to turn feature names into columns\n",
    ")\n",
    "\n",
    "# Rename columns for clarity\n",
    "high_corr_pairs.columns = [\"Feature_1\", \"Feature_2\", \"Correlation\"]\n",
    "\n",
    "# Filter pairs with high absolute correlation\n",
    "high_corr_pairs = high_corr_pairs[\n",
    "    high_corr_pairs[\"Correlation\"].abs() >= threshold\n",
    "].sort_values(\n",
    "    by=\"Correlation\",\n",
    "    ascending=False\n",
    ")\n",
    "# Sort pairs by correlation strength (highest first)\n",
    "\n",
    "\n",
    "high_corr_pairs\n",
    "# Display highly correlated feature pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b277c59-a19a-49d2-9fdd-56a27ed5c92d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 1. Recreate df_corr (INCLUDING target)\n",
    "# ============================\n",
    "\n",
    "# Create a copy of the cleaned dataset for correlation analysis\n",
    "df_corr = df_clean.copy()\n",
    "\n",
    "\n",
    "# Combine high-cardinality categorical columns with the original target column\n",
    "exclude_cols = cat_but_car + [\"Attrition\"]\n",
    "\n",
    "# Drop selected columns safely (ignore if the column does not exist)\n",
    "df_corr = df_corr.drop(\n",
    "    columns=[col for col in exclude_cols if col in df_corr.columns],\n",
    "    errors='ignore'\n",
    ")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 2. One-Hot Encoding (nominal categorical features)\n",
    "# ============================\n",
    "\n",
    "# Select object-type columns except numeric-but-categorical ones\n",
    "ohe_cols = [\n",
    "    col for col in df_corr.select_dtypes(include='object').columns\n",
    "    if col not in num_but_cat\n",
    "]\n",
    "\n",
    "# Print columns that will be one-hot encoded\n",
    "print(f\"Columns to be one-hot encoded: {ohe_cols}\")\n",
    "\n",
    "# Apply one-hot encoding and avoid the dummy variable trap\n",
    "df_corr = pd.get_dummies(\n",
    "    df_corr,\n",
    "    columns=ohe_cols,\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 3. Correlation with target (ABSOLUTE VALUE + SORTED)\n",
    "# ============================\n",
    "\n",
    "# Check if the target column exists\n",
    "if 'Attrition_num' not in df_corr.columns:\n",
    "    \n",
    "    # Warn if the target column is missing\n",
    "    print(\"ERROR: Attrition_num column is missing! Creating it from df_clean.\")\n",
    "    \n",
    "    # Add the target column from df_clean\n",
    "    df_corr['Attrition_num'] = df_clean['Attrition_num']\n",
    "    \n",
    "    # Confirm that the target column was added\n",
    "    print(\"Attrition_num column has been added.\")\n",
    "\n",
    "\n",
    "# Compute Pearson correlation with the target (exclude self-correlation)\n",
    "target_corr_series = (\n",
    "    df_corr.corr(method=\"pearson\")['Attrition_num']\n",
    "    .drop('Attrition_num')\n",
    ")\n",
    "\n",
    "\n",
    "# Sort features by absolute correlation strength\n",
    "target_corr = (\n",
    "    target_corr_series\n",
    "    .abs()\n",
    "    .sort_values(ascending=False)\n",
    "    .round(4)\n",
    ")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 4. Heatmap (Horizontal - Best visualization)\n",
    "# ============================\n",
    "\n",
    "# Create a wide figure for better readability\n",
    "plt.figure(figsize=(30, 6))\n",
    "\n",
    "# Draw heatmap for correlations with the target\n",
    "sns.heatmap(\n",
    "    # Convert Series to a single-row DataFrame\n",
    "    target_corr.to_frame().T,\n",
    "    \n",
    "    # Use a diverging color map\n",
    "    cmap=\"coolwarm\",\n",
    "    \n",
    "    # Center the color scale at zero\n",
    "    center=0,\n",
    "    \n",
    "    # Display correlation values\n",
    "    annot=True,\n",
    "    \n",
    "    # Format annotation numbers\n",
    "    fmt=\".3f\",\n",
    "    \n",
    "    # Customize annotation style\n",
    "    annot_kws={\"size\": 11, \"weight\": \"bold\"},\n",
    "    \n",
    "    # Draw grid lines\n",
    "    linewidths=0.3,\n",
    "    linecolor=\"white\",\n",
    "    \n",
    "    # Customize the color bar\n",
    "    cbar_kws={\"shrink\": 0.9, \"label\": \"Correlation with Attrition_num\"}\n",
    ")\n",
    "\n",
    "# Set the heatmap title\n",
    "plt.title(\n",
    "    \"Correlation of Features with Attrition_num (Absolute Value | Sorted)\",\n",
    "    fontsize=16,\n",
    "    fontweight=\"bold\",\n",
    "    pad=20\n",
    ")\n",
    "\n",
    "# Label the x-axis\n",
    "plt.xlabel(\"Features (Strongest â†’ Weakest)\", fontsize=15)\n",
    "\n",
    "# Label the y-axis\n",
    "plt.ylabel(\"Attrition_num (Target)\", fontsize=15)\n",
    "\n",
    "# Rotate feature names for readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Keep the target label horizontal\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 5. Table + TOP 20 features\n",
    "# ============================\n",
    "\n",
    "# Print separator\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Print section title\n",
    "print(\"TOP 20 FEATURES MOST CORRELATED WITH ATTRITION_NUM\")\n",
    "\n",
    "# Print separator\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select the top 20 strongest correlated features\n",
    "top_20 = target_corr.head(20)\n",
    "\n",
    "# Display the top 20 features\n",
    "print(top_20)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Positive vs Negative correlation split\n",
    "# ============================\n",
    "\n",
    "# Header for positive correlations\n",
    "print(\"\\nPOSITIVE CORRELATION (Increases attrition risk):\")\n",
    "\n",
    "# Show strongest positive correlations\n",
    "print(target_corr_series[top_20.index].head(10))\n",
    "\n",
    "# Header for negative correlations\n",
    "print(\"\\nNEGATIVE CORRELATION (Decreases attrition risk):\")\n",
    "\n",
    "# Show strongest negative correlations\n",
    "print(target_corr_series[top_20.index].tail(10))\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 6. Bar plot (Bonus visualization)\n",
    "# ============================\n",
    "\n",
    "# Create a figure for the bar plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Select top 15 features by absolute correlation\n",
    "top_20_abs = target_corr.head(15)\n",
    "\n",
    "# Assign colors based on correlation direction\n",
    "colors = [\n",
    "    'red' if target_corr_series[name] > 0 else 'blue'\n",
    "    for name in top_20_abs.index\n",
    "]\n",
    "\n",
    "# Plot a horizontal bar chart\n",
    "top_20_abs.plot(\n",
    "    kind='barh',\n",
    "    color=colors,\n",
    "    alpha=0.8,\n",
    "    figsize=(12, 8)\n",
    ")\n",
    "\n",
    "# Set the plot title\n",
    "plt.title(\n",
    "    'Top 15 Strongest Features - Correlation with Attrition (|value|)',\n",
    "    fontsize=16\n",
    ")\n",
    "\n",
    "# Label the x-axis\n",
    "plt.xlabel('Correlation Strength (Absolute Value)', fontsize=12)\n",
    "\n",
    "# Label the y-axis\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "\n",
    "# Show the strongest feature at the top\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Add grid lines\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Adjust spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Final summary\n",
    "# ============================\n",
    "\n",
    "# Print the total number of features (excluding the target)\n",
    "print(f\"\\nâœ… Total number of features: {len(df_corr.columns) - 1}\")\n",
    "\n",
    "# Print the strongest correlated feature with the target\n",
    "print(f\"âœ… Highest correlation: {target_corr.iloc[0]:.3f} ({top_20.index[0]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed6f908-7b2d-480a-892e-9e0b6d7b738b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Feature Engineering by Combining Highly Correlated Variables\n",
    "# ============================================================\n",
    "\n",
    "# Create a copy of the cleaned dataset to avoid modifying the original\n",
    "df_clean_ = df_clean.copy()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Normalize income and experience by job level\n",
    "#    (Reduces multicollinearity and captures relative strength)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Monthly income normalized by job level\n",
    "df_clean_['IncomePerLevel'] = df_clean_['MonthlyIncome'] / df_clean_['JobLevel']\n",
    "\n",
    "# Total working years normalized by job level\n",
    "df_clean_['ExperiencePerLevel'] = df_clean_['TotalWorkingYears'] / df_clean_['JobLevel']\n",
    "\n",
    "# Income earned per year of experience\n",
    "# +1 added to avoid division by zero\n",
    "df_clean_['IncomePerExperienceYear'] = (\n",
    "    df_clean_['MonthlyIncome'] / (df_clean_['TotalWorkingYears'] + 1)\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Stability and tenure-related features\n",
    "#    (Captures role and manager consistency)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Average stability between current role and current manager\n",
    "df_clean_['RoleManagerStability'] = (\n",
    "    df_clean_['YearsInCurrentRole'] + df_clean_['YearsWithCurrManager']\n",
    ") / 2\n",
    "\n",
    "# Ratio of time spent with current manager relative to company tenure\n",
    "df_clean_['ManagerTenureRatio'] = (\n",
    "    df_clean_['YearsWithCurrManager'] / (df_clean_['YearsAtCompany'] + 1)\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Promotion delay features\n",
    "#    (Measures career stagnation risk)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Promotion delay normalized by total years at the company\n",
    "df_clean_['PromotionLag'] = (\n",
    "    df_clean_['YearsSinceLastPromotion'] / (df_clean_['YearsAtCompany'] + 1)\n",
    ")\n",
    "\n",
    "# Absolute promotion delay kept as an independent signal\n",
    "df_clean_['YearsSinceLastPromotionNormalized'] = df_clean_['YearsSinceLastPromotion']\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Overtime and commuting burden features\n",
    "#    (Captures workload and stress effects)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Convert OverTime from categorical to numerical\n",
    "df_clean_['OverTimeNum'] = df_clean_['OverTime'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Combined effect of overtime and commuting distance\n",
    "df_clean_['OvertimeBurden'] = (\n",
    "    df_clean_['OverTimeNum'] * df_clean_['DistanceFromHome']\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Business travel intensity and commute stress\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Encode business travel frequency ordinally\n",
    "travel_map = {\n",
    "    'Non-Travel': 0,\n",
    "    'Travel_Rarely': 1,\n",
    "    'Travel_Frequently': 2\n",
    "}\n",
    "df_clean_['BusinessTravelNum'] = df_clean_['BusinessTravel'].map(travel_map)\n",
    "\n",
    "# Combined effect of travel frequency and commuting distance\n",
    "df_clean_['CommuteStress'] = (\n",
    "    df_clean_['DistanceFromHome'] * df_clean_['BusinessTravelNum']\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. Overall satisfaction score\n",
    "#    (Aggregates multiple satisfaction dimensions)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Mean of job, environment, and relationship satisfaction\n",
    "df_clean_['OverallSatisfaction'] = (\n",
    "    df_clean_['JobSatisfaction']\n",
    "    + df_clean_['EnvironmentSatisfaction']\n",
    "    + df_clean_['RelationshipSatisfaction']\n",
    ") / 3\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7. Drop original variables after feature engineering\n",
    "#    (Prevents redundancy and multicollinearity)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "to_drop_after_fe = [\n",
    "    'MonthlyIncome',             # Replaced by income-normalized features\n",
    "    'TotalWorkingYears',         # Used in experience-based ratios\n",
    "    'JobLevel',                  # Used for normalization\n",
    "    'YearsInCurrentRole',        # Included in stability feature\n",
    "    'YearsWithCurrManager',      # Included in stability & ratio features\n",
    "    'YearsSinceLastPromotion',   # Normalized and preserved separately\n",
    "    'OverTime',                  # Converted to numerical form\n",
    "    'DistanceFromHome',          # Used in stress-related features\n",
    "    'BusinessTravel',            # Converted to ordinal numeric form\n",
    "    'JobSatisfaction',           # Included in OverallSatisfaction\n",
    "    'EnvironmentSatisfaction',   # Included in OverallSatisfaction\n",
    "    'RelationshipSatisfaction'   # Included in OverallSatisfaction\n",
    "]\n",
    "\n",
    "# Drop redundant columns after feature engineering\n",
    "df_model = df_clean_.drop(columns=to_drop_after_fe)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8. Final dataset summary\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Print number of dropped columns\n",
    "print(f\"Number of dropped columns: {len(to_drop_after_fe)}\")\n",
    "\n",
    "# Print remaining feature count\n",
    "print(f\"Remaining number of features: {df_model.shape[1]}\")\n",
    "\n",
    "# Print remaining feature names\n",
    "print(\"Remaining key features:\", df_model.columns.tolist())\n",
    "\n",
    "# Display final modeling dataset\n",
    "df_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c529127",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069539f3-d484-47a9-8a46-748cf6c8a252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Column Type Inspection (FOR CONTROL PURPOSES ONLY)\n",
    "# ============================================================\n",
    "\n",
    "# Re-check column types after feature engineering\n",
    "# cat_cols, cat_but_car, num_cols, num_but_cat = grab_col_names(df_model, verbose=True)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Ensure correct target handling\n",
    "#    - Drop string target if exists\n",
    "#    - Attrition_num MUST remain\n",
    "# ============================================================\n",
    "\n",
    "# Drop original string target if still present\n",
    "if 'Attrition' in df_model.columns:\n",
    "    df_model = df_model.drop(columns=['Attrition'])\n",
    "\n",
    "# Ensure numerical target exists\n",
    "if 'Attrition_num' not in df_model.columns:\n",
    "    df_model['Attrition_num'] = df_clean['Attrition_num']\n",
    "    print(\"Attrition_num was missing and has been added.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Select categorical columns for One-Hot Encoding\n",
    "#    (Only object-type features, excluding the target)\n",
    "# ============================================================\n",
    "\n",
    "categorical_cols = (\n",
    "    df_model\n",
    "    .select_dtypes(include='object')\n",
    "    .columns\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "print(f\"Columns to be one-hot encoded: {categorical_cols}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. One-Hot Encoding\n",
    "#    - drop_first=True to avoid dummy variable trap\n",
    "# ============================================================\n",
    "\n",
    "df_final = pd.get_dummies(\n",
    "    df_model,\n",
    "    columns=categorical_cols,\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Split features (X) and target (y)\n",
    "# ============================================================\n",
    "\n",
    "# Safety check for target column\n",
    "if 'Attrition_num' not in df_final.columns:\n",
    "    raise ValueError(\"Attrition_num column is missing after encoding!\")\n",
    "\n",
    "X = df_final.drop('Attrition_num', axis=1)\n",
    "y = df_final['Attrition_num']\n",
    "\n",
    "print(f\"Final number of features: {X.shape[1]}\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. Train-Test Split\n",
    "#    - Stratified to preserve class imbalance\n",
    "# ============================================================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.30,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Train size: {X_train.shape[0]} samples, \"\n",
    "    f\"Test size: {X_test.shape[0]} samples\"\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. Feature Scaling\n",
    "#    - Required for Logistic Regression\n",
    "# ============================================================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. Logistic Regression Model\n",
    "#    - Class imbalance handled with class_weight='balanced'\n",
    "# ============================================================\n",
    "\n",
    "log_reg = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=1.0,\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. Predictions and Evaluation Metrics\n",
    "# ============================================================\n",
    "\n",
    "# Class predictions\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Predicted probabilities for ROC-AUC\n",
    "y_pred_proba = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" LOGISTIC REGRESSION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9. Model Coefficients Interpretation\n",
    "#    - Sorted by absolute effect size\n",
    "# ============================================================\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': log_reg.coef_[0]\n",
    "}).sort_values(by='Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nðŸ“Š Top 15 Most Influential Features:\")\n",
    "display(coef_df.head(15).round(4))\n",
    "\n",
    "print(\"\\nðŸ“ˆ Features Increasing Attrition Risk:\")\n",
    "display(coef_df[coef_df['Coefficient'] > 0].head(10).round(4))\n",
    "\n",
    "print(\"\\nðŸ“‰ Features Decreasing Attrition Risk:\")\n",
    "display(coef_df[coef_df['Coefficient'] < 0].head(10).round(4))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 10. ROC Curve Visualization\n",
    "# ============================================================\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(\n",
    "    fpr,\n",
    "    tpr,\n",
    "    lw=2,\n",
    "    label=f'ROC Curve (AUC = {roc_auc_score(y_test, y_pred_proba):.3f})'\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], lw=2, linestyle='--')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Logistic Regression')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01be73a4-e565-4d4a-a89a-9664b9611dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
